{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IGNORE FOLLOWING CODE, JUST EXECTURE IT AND GO TO BLOCK BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "class DownloadData(): \n",
    "    \"\"\"\n",
    "    Collection of methods that allows to donwload historical data trough Dukascopy API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ticker, start_date, end_date,timeframe):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ticker : int\n",
    "            Desired forex pair with format \"XXX/YYY\"\n",
    "        start_date : str\n",
    "            Start date with format \"DD/MM/YYYY\"\n",
    "        end_date : str\n",
    "            End date with format \"DD/MM/YYYY\"\n",
    "        \"\"\"\n",
    "        timeframes = ['1day', '1hour', '10m','1min', '10sec', 'tick']\n",
    "        if timeframe not in timeframes:\n",
    "            raise SystemExit(f\"Invalid timeframe, valid inputs are {timeframes}\")\n",
    "        else:\n",
    "            self.timeframe = timeframe\n",
    "\n",
    "        self.api_key = 'z3pstgimi8000000'\n",
    "        self.ticker = ticker\n",
    "        self.symbol = self.getTicker()\n",
    "        \n",
    "        # Date must be converted to UNIX with milliseconds\n",
    "        self.start_date = datetime.timestamp(datetime.strptime(start_date, \"%d-%m-%Y\").replace(tzinfo=timezone.utc)) * 1000\n",
    "        self.end_date = datetime.timestamp(datetime.strptime(end_date, \"%d-%m-%Y\").replace(tzinfo=timezone.utc)) * 1000\n",
    "\n",
    "        if self.start_date > self.end_date:\n",
    "            raise SystemExit(f\"Start date can't be after end date.\")\n",
    "\n",
    "    def getTicker(self):\n",
    "        url = 'https://freeserv.dukascopy.com/2.0/?path=api/instrumentList'\n",
    "        params = {'key': self.api_key}\n",
    "    \n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = pd.DataFrame(response.json())\n",
    "            filtered_data = data[data['name'] == self.ticker]\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "        if not filtered_data.empty:\n",
    "            found_id = filtered_data['id'].values[0]\n",
    "            del data, filtered_data, url, params, response\n",
    "            return found_id\n",
    "        else:\n",
    "            raise SystemExit(f\"No data found for name: {self.ticker}\")\n",
    "\n",
    "    def getData(self):\n",
    "\n",
    "        url = 'https://freeserv.dukascopy.com/2.0/?path=api/historicalPrices'\n",
    "\n",
    "        download_progress = None\n",
    "        final_data = pd.DataFrame()\n",
    "        attempt = 0\n",
    "        '''\n",
    "        I had to create a while loop because the API let me download max. only 5000 rows of data at once. \n",
    "        So I create a loop that will do multiple requests, basically it donwload 5k rows of data at time \n",
    "        until all data relative to the desired datetime range has been donwloaded.\n",
    "        '''\n",
    "        while(True):\n",
    "            '''\n",
    "            If I want data from 01/01/2023 to today, then I'll be given only the most recent 5k rows of data. \n",
    "            Then I check if I already downloaded data into the dataframe, if yes less_recent_value assumes \n",
    "            the value of the first row Timestamp, so that this cycle will download the 5k rows of data that \n",
    "            precedes the ones already downlaoded. \n",
    "            '''\n",
    "            params = {\n",
    "                'key': self.api_key,\n",
    "                'instrument': self.symbol,\n",
    "                'timeFrame': self.timeframe,\n",
    "                'count' : 5000, # Max rows of data downloadable, can't insert a value higher than this. \n",
    "                'start' : self.start_date,\n",
    "                'end' : self.end_date if download_progress is None else download_progress\n",
    "            }\n",
    "\n",
    "            attempt = attempt+1\n",
    "            print(f'\\r \\t Downloading... {self.ticker} '\n",
    "                 f'{datetime.utcfromtimestamp(params[\"end\"] / 1000)} '\n",
    "                 f'Call n. {attempt}', end='', flush=True)\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = pd.DataFrame(response.json())\n",
    "\n",
    "                '''               \n",
    "                There's a bug in the API: if you do rapidly many requests it can happen thtat\n",
    "                you get data for an incorrect instrument. So this condition makes sure that\n",
    "                the API provided the requested data, if not the code will continue to do\n",
    "                requests until API deliever correct data. \n",
    "                '''\n",
    "\n",
    "                if not data[data['id'] != self.symbol].empty:\n",
    "                    continue \n",
    "                \n",
    "                # Converting json in dataframe\n",
    "                if params['timeFrame'] == 'tick':\n",
    "                   data = pd.DataFrame(data['ticks'].apply(lambda x: {'Timestamp': x['timestamp'],'Bid': x['bid'],\n",
    "                                                                       'Ask': x['ask']}).tolist())\n",
    "                else:\n",
    "                   data = pd.DataFrame(data['candles'].apply(lambda x: {'Timestamp': x['timestamp'],'Open': x['bid_open'], \n",
    "                                                                         'High': x['bid_high'], 'Low': x['bid_low'], \n",
    "                                                                         'Close': x['bid_close']}).tolist())\n",
    "\n",
    "                # Appending data retrieved on this call to the main dataframe\n",
    "                final_data = pd.concat([data, final_data], ignore_index=True)\n",
    "            else:\n",
    "                raise SystemExit(f\"Request failed with status code: {response.status_code}\")\n",
    "            \n",
    "            new_progress = final_data['Timestamp'].iloc[0]\n",
    "            if not download_progress == new_progress: \n",
    "               download_progress = new_progress\n",
    "            else:\n",
    "               break\n",
    "        \n",
    "        # Clear print output\n",
    "        print('\\r' + ' ' * 80, end='\\r', flush=True)\n",
    "\n",
    "        # Check if there are duplicated rows\n",
    "        if not final_data[final_data.duplicated()].empty:\n",
    "            print(\"There are duplicated rows in the combined DataFrame.\")\n",
    "\n",
    "        # Convert timestamp to human-readable date and setting it as index\n",
    "        final_data['Timestamp'] = pd.to_datetime(final_data['Timestamp'], unit='ms',utc=True)\n",
    "        final_data.set_index('Timestamp',inplace=True)\n",
    "        \n",
    "        return final_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (start here) Classify price movements based on candlesticks statistics\n",
    "\n",
    "This reseach comes from [this](https://www.forexfactory.com/thread/post/14707863#post14707863) post on ForexFactory. \n",
    "\n",
    "### Step 1: gather data and create a rolling TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allow jupiter to upload in real time externally modified code\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"01-12-2023\"\n",
    "end_date = \"10-12-2023\" #This code being executed on google servers make everything slow. \n",
    "                        #Do not download too much data or it will take much time\n",
    "timeframe = 'tick'\n",
    "price_frame = 1000\n",
    "\n",
    "csv_file_path = f\"{os.getcwd()}/x_DATA/{start_date}_{end_date}  {timeframe}.csv\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "if os.path.exists(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "else:\n",
    "    df = DownloadData('GBP/USD', start_date,end_date,timeframe).getData()\n",
    "    df.reset_index(inplace=True)    \n",
    "    data_folder_path = f\"{os.getcwd()}/x_DATA\"\n",
    "    if not os.path.exists(data_folder_path):\n",
    "        os.makedirs(data_folder_path)\n",
    "\n",
    "    df.to_csv(csv_file_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Ask\" in df.columns:\n",
    "    df = df.drop(\"Ask\", axis=1)\n",
    "\n",
    "df[\"Close\"] = df[\"Bid\"]\n",
    "df[\"Open\"] = df[\"Bid\"].shift(price_frame)  # Shift the \"Bid\" values 1000 rows back\n",
    "df[\"High\"] = df[\"Bid\"].rolling(window=price_frame).max()  # Calculate the rolling max over the last 1000 rows\n",
    "df[\"Low\"] = df[\"Bid\"].rolling(window=price_frame).min()  # Calculate the rolling min over the last 1000 rows\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time of high and time of low\n",
    "I'm adding a column that returns the high and low times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the index of the maximum value in the rolling window for \"High time\"\n",
    "df['High time'] = price_frame - ( df.index.values - df['Bid'].rolling(window=price_frame).agg(lambda x: x.index.values[np.argmax(x.values)]) ) \n",
    "df[\"Low time\"] = price_frame  - ( df.index.values - df['Bid'].rolling(window=price_frame).agg(lambda x: x.index.values[np.argmin(x.values)]) )\n",
    "df['High first'] = df['High time'] > df[\"Low time\"]\n",
    "\n",
    "df = df.dropna() #Drop initial NaN values\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpret 'High time' and 'Low time' columns\n",
    "e.g. \n",
    "- High time = 746 and Low time = 200\n",
    "- That means: the high was made on tick number 746 of the current 1000 ticks candlestick\n",
    "\n",
    "Here's a draw so that you can fully understand\n",
    "![Drawing](https://i.imgur.com/ORcChmJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Directional bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open - Close'] = (df['Open'] - df['Close']).abs()\n",
    "df['Upper Wick'] = (df['High'] - df[['Open', 'Close']].max(axis=1)).abs()\n",
    "df['Lower Wick'] = (df[['Open', 'Close']].min(axis=1) - df['Low']).abs()\n",
    "\n",
    "df['Bias'] = np.where(df['Open'] > df['Close'], 'Bearish', np.where(df['Close'] > df['Open'], 'Bullish', 'Doji'))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Strength of directional bias\n",
    "It can be:\n",
    "- weak\n",
    "- medium (\"normal\")\n",
    "- strong\n",
    "\n",
    "I'll calculate the average candlestick like: $\\frac{\\text{Average Bullish candlestick OC}+\\text{Average Bearish candlestick OC}}{2}$. \n",
    "Then I'll calculate the standard deviation of the bullish candles OC and bearish candles OC and then: $\\frac{\\text{St.dev Bullish candlestick OC}+\\text{St.dev Bearish candlestick OC}}{2}$. \n",
    "And lastly I'll create a boundary around the average, meaning that:\n",
    "- If a bullish candle OC is $>=\\mu+\\sigma$ then it's strong. \n",
    "- If a bullish candle OC is $>=\\mu$ and $<\\mu+\\sigma$ then it's medium (\"normal\").\n",
    "- If a bullish candle OC is $<=\\mu-\\sigma$ then it's weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter rows where Bias is \"Bullish\"\n",
    "bullish_rows = df[df['Bias'] == 'Bullish']\n",
    "bearish_rows = df[df['Bias'] == 'Bearish']\n",
    "\n",
    "# average bullish and bearish candle OC\n",
    "average_bullish_diff = bullish_rows['Open - Close'].mean()\n",
    "average_bearish_diff = bearish_rows['Open - Close'].mean()\n",
    "mu = ( average_bullish_diff + average_bearish_diff ) / 2\n",
    "\n",
    "# stdev bullish and bearish candle OC\n",
    "stdev_bullish_diff = bullish_rows['Open - Close'].std()\n",
    "stdev_bearish_diff = bearish_rows['Open - Close'].std()\n",
    "sigma = ( stdev_bullish_diff + stdev_bearish_diff ) / 2\n",
    "\n",
    "\n",
    "# assign strenght bias\n",
    "df['Strength'] = np.where(df['Open - Close'] >= mu + sigma, 'Strong',\n",
    "                          np.where((df['Open - Close'] > mu-sigma) & (df['Open - Close'] < mu + sigma), 'Medium', 'Weak'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible values in each column\n",
    "unique_biases = df['Bias'].unique()\n",
    "unique_strengthes = df['Strength'].unique()\n",
    "unique_high_first_values = df['High first'].unique()\n",
    "\n",
    "# Create a list to store the results\n",
    "result_data = []\n",
    "\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "# Iterate over combinations and calculate mean\n",
    "for bias in unique_biases:\n",
    "    for strength in unique_strengthes:\n",
    "        for high_first in unique_high_first_values:\n",
    "            subset = df[(df['Bias'] == bias) & (df['Strength'] == strength) & (df['High first'] == high_first)]\n",
    "            mean_oc = subset['Open - Close'].mean()\n",
    "            mean_oc = round(mean_oc,5)\n",
    "            result_data.append({'Bias': bias, 'Strength': strength, 'High First': high_first, 'Avg. Open - Close': mean_oc})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "result_df = pd.DataFrame(result_data)\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(result_df, headers='keys', tablefmt='simple_outline', showindex=False, floatfmt=\".5f\", numalign=\"center\", stralign=\"center\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
